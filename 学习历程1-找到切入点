背景：已经看了一部分的文档，并且也使用scrapy抓取了一些简单的内容。

但是使用了scrapy之后还是有相当的疑惑。我们在学习爬虫的理论时，明明涉及到很多的理论知识，比如神马待爬队列啦，爬取策略啦，去重啦。在使用scrapy时呢，这些都没有涉及到，定义好Item，完成Spider的Parse方法，咱们的爬虫就能跑起来。说实话，虽然说这样就跑起来了，数据神马的也没有问题。但是心里还是虚的慌，所以还是觉得要深入的学习一下。

在下载到了代码之后，马上开始进行。

第一步：找到入口。我在代码里面找了很久，才找到入口，智商捉鸡呀。
运行scrapy使用的命令是scrapy crawl spidername
所以入口脚本应该与crawl指令有关，于是去scrapy/commands/文件夹下面找了一圈，死活没找到。
就开始去其他源码里面找，找了大半天，没有什么实质性的进展。
后来觉得是不是自己太过于浮躁，于是静下心来又去官方文档里面一个一个专题的读文档。
在看到“核心API”这章的时候，里面赫然写着“Scrapy API的主要入口是 Crawler 的实例对象”。

哎呀，我怎么这么二缺，我为啥不在文档里面搜索一下crawl这个关键词呀，明明就找对方向的说。
为了完成这第一步，已经过去两三天了。不过还好，对于文档中的内容有了进一步的了解。

第二步：找到引擎的位置
在看了官网的架构图之后，知道了几个核心的模块，处于中心的是引擎模块，所以找到引擎模块的代码，应该就能确定其余几个模块的代码在那里。
scrapy/crawler.py中有如下导入：
from scrapy.core.engine import ExecutionEngine
所以引擎在scrapy/core/enging.py文件夹下
进入core文件夹，看到engine.py,scheduler.py,scraper.py,spidermw.py，downloader/。
对比文档中提到的架构图，引擎，调度器，蜘蛛，pipeline，下载器。core文件夹下包括了其中四样，那么毫无疑问，core中就scrapy这个框架最核心的功能模块。（文件夹起名叫core的也少有不是核心的~）


第三步，学习核心代码。
我给自己定了一个原则：从最基本的模块开始学习。因为像engine这样的模块，必然是相当高层的模块，直接学习基本上不大可能，向下牵涉到的模块众多，根本难以缕清思路。
所以要从引擎开始，向下层模块挖掘，找到对于其他模块依赖较少的模块，就学习。如果一个模块中，所有依赖的模块都学习完了，就开始学习当前模块。
学习笔记详见对应的.py文件。
